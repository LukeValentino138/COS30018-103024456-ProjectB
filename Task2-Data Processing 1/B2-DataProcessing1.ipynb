{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do for B.2 -\n",
    "1. Write a function to load and process a dataset with multiple features with the following requirements:\n",
    "    a. This function will allow you to specify the start date and the end date for the whole dataset as inputs.\n",
    "\n",
    "    b. This function will allow you to deal with the NaN issue in the data.\n",
    "\n",
    "    c. This function will also allow you to use different methods to split the data into train/test data; e.g. you can split it according to some specified ratio of train/test and you can specify to split it by date or randomly.\n",
    "\n",
    "    d. This function will have the option to allow you to store the downloaded data on your local machine for future uses and to load the data locally to save time.\n",
    "    \n",
    "    e. This function will also allow you to have an option to scale your feature columns and store the scalers in a data structure to allow future access to these scalers.\n",
    "\n",
    "The goal for this task is to just process the data, NOT to train the model aswell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas_datareader as web\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, InputLayer\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "company_ticker = \"AMZN\"\n",
    "\n",
    "# Specification of start and end date for both train and test sets\n",
    "# This means that at some point we need to split the dataset\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "NaNHandler = \"drop\" # Determines how NaN data is handled. Can be \"drop\", \"fill_mean\", \"fill_median\", or \"fill_ffill\"\n",
    "\n",
    "test_size = 0.25 # e.g. this would be that 75% of the available data is being used for the train set and 25% is being used for the test set.\n",
    "\n",
    "# If false the data will be randomly split\n",
    "split_by_date = True\n",
    "\n",
    "feature_column = [\"Open\", \"Adj Close\"] # Can be set to \"Adj Close\", \"Volume\", \"Open\", \"High\", or \"Low\"\n",
    "\n",
    "# Scalar is used to normalize data to a specific range (In this code, it normalizes the data to between 0 and 1)\n",
    "scale = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data from Yahoo Finance or Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_save_data(company_ticker, start_date, end_date, base_dir=\"data\"):\n",
    "    ###\n",
    "    # Loads the dataset for the input company and range.\n",
    "    # If the dataset is not available locally, it downloads the data and saves it as a CSV file.\n",
    "    # Parameters:\n",
    "    # - company_ticker: ticker of the company (e.g., \"AMZN\")\n",
    "    # - start_date: the start date for the dataset in 'YYYY-MM-DD' format\n",
    "    # - end_date: the end date for the dataset in 'YYYY-MM-DD' format\n",
    "    # - base_dir: the base directory where the data will be saved, default is set to \"data\"\n",
    "    # Returns:\n",
    "    # - data: pandas DF, the loaded dataset with the specified features\n",
    "    ###\n",
    "\n",
    "    # Generate the save path based on ticker and date range\n",
    "    filename = f\"{company_ticker}_{start_date}_to_{end_date}.csv\"\n",
    "    save_path = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(save_path):\n",
    "        # Load the dataset from the local file\n",
    "        data = pd.read_csv(save_path, index_col=0, parse_dates=True)\n",
    "        print(f\"Data loaded from local file: {save_path}\")\n",
    "    else:\n",
    "        # If the file doesn't exist, download the data\n",
    "        data = yf.download(company_ticker, start=start_date, end=end_date)\n",
    "        \n",
    "        # Make sure the base directory exists\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the dataset locally\n",
    "        data.to_csv(save_path)\n",
    "        print(f\"Data downloaded and saved locally to: {save_path}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nan(data, method='drop'):\n",
    "    ###\n",
    "    # Handles NaN values in the dataset based on the specified method.\n",
    "    # Parameters:\n",
    "    # - data: pandas Dataframe\n",
    "    # - method: str, how to handle NaN values. Options are 'drop', 'fill_mean', 'fill_median', 'fill_ffill'.\n",
    "    # Drop removes all NaN data from the dataset.\n",
    "    # Mean replaces the NaN data with the mean average of all the data \n",
    "    # Mediam replaces the NaN data with the median average of all the data \n",
    "    # FFill sets the NaN data to the most recent valid data\n",
    "    # Returns:\n",
    "    # - data: pandas Dataframe, the dataset with NaN values handled\n",
    "    ###\n",
    "\n",
    "\n",
    "    if method == 'drop':\n",
    "        data = data.dropna()\n",
    "    elif method == 'fill_mean':\n",
    "        data = data.fillna(data.mean())\n",
    "    elif method == 'fill_median':\n",
    "        data = data.fillna(data.median())\n",
    "    elif method == 'fill_ffill':\n",
    "        data = data.fillna(method='ffill')\n",
    "    else:\n",
    "        raise ValueError(\"Choose from 'drop', 'fill_mean', 'fill_median', 'fill_ffill'.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.25, split_by_date=True, date_column='Date'):\n",
    "\n",
    "    ###\n",
    "    # Splits the dataset into training and testing sets based on the specified methods.\n",
    "    # Parameters:\n",
    "    # - data: pandas datadframe, the dataset to split\n",
    "    # - test_size: float, the amount of the dataset to include in the test split (default is 0.25)\n",
    "    # - split_by_date: bool, split the data by date (True) or randomly (False). If false, the data is split using sklearns train_test_split method\n",
    "    # - date_column: str, the name of the date column to use for date-based splitting (only needed if split_by_date=True)\n",
    "    # Returns:\n",
    "    # - train_data: training set as a pandas dataframe\n",
    "    # - test_data: testing set as a pandas dataframe\n",
    "    #\n",
    "    #\n",
    "    ###\n",
    "\n",
    "    if split_by_date:\n",
    "        # Sort data by date\n",
    "        data = data.sort_values(by=date_column)\n",
    "        \n",
    "        # Determine the split index\n",
    "        split_index = int(len(data) * (1 - test_size))\n",
    "        \n",
    "        # Split the data\n",
    "        train_data = data.iloc[:split_index]\n",
    "        test_data = data.iloc[split_index:]\n",
    "    else:\n",
    "        # Randomly split the data using sklearn's train_test_split\n",
    "        train_data, test_data = train_test_split(data, test_size=test_size, shuffle=True, random_state=180)\n",
    "    \n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_data(data, feature_columns):\n",
    "    ###\n",
    "    # Scales the specified feature columns in the dataset using MinMaxScaler and stores the scalers.\n",
    "    # Parameters:\n",
    "    # - data: pandas dataframe, the dataset to scale\n",
    "    # - feature_columns: list, a list of feature columns to scale (e.g., [\"Adj Close\"])\n",
    "    # Returns:\n",
    "    # - scaled_data: pandas dataframe, the dataset with scaled feature columns\n",
    "    # - scalers: dict, a dictionary of scalers used to scale the feature columns\n",
    "    ###\n",
    "    scalers = {}\n",
    "    scaled_data = data.copy()\n",
    "    \n",
    "    # scale data in specified feature columns\n",
    "    for feature in feature_columns:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data[feature] = scaler.fit_transform(scaled_data[[feature]])\n",
    "        scalers[feature] = scaler  # Store the scaler for future access\n",
    "    \n",
    "    return scaled_data, scalers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using functionalities with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from local file: data/AMZN_2018-01-01_to_2024-01-01.csv\n",
      "                Open       High        Low      Close  Adj Close    Volume\n",
      "Date                                                                      \n",
      "2018-01-02  0.000000  59.500000  58.525501  59.450500   0.000000  53890000\n",
      "2018-01-03  0.006337  60.274502  59.415001  60.209999   0.005975  62176000\n",
      "2018-01-04  0.012830  60.793499  60.233002  60.479500   0.008095  60442000\n",
      "2018-01-05  0.017694  61.457001  60.500000  61.457001   0.015784  70894000\n",
      "2018-01-08  0.024883  62.653999  61.601501  62.343498   0.022758  85590000\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = load_or_save_data(company_ticker, start_date, end_date)\n",
    "\n",
    "# Handle NaNs\n",
    "data = handle_nan(data, NaNHandler)\n",
    "\n",
    "# Split the data \n",
    "train_data, test_data = split_data(data, test_size, split_by_date)\n",
    "\n",
    "# Scale the data\n",
    "scaled_train_data, scalers = scale_data(train_data, feature_column)\n",
    "\n",
    "# Show data\n",
    "print(scaled_train_data.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
